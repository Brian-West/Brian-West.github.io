---
title: OpenCV中SIFT,SURF,ORB,kd-tree,lsh,RANSAC算法记录（仅作为个人理解）
---

## kd-tree。
### 参考：https://www.cnblogs.com/lysuns/articles/4710712.html
1. 本质上跟BST差不多，构建的过程中如果比当前节点小则进入左子树，大则进入右子树。选择某一个维度Di，然后比较两个K维数在该维度 Di上的大小关系，即每次选择一个维度Di来对K维数据进行划分，相当于用一个垂直于该维度Di的超平面将K维数据空间一分为二，平面一边的所有K维数据 在Di维度上的值小于平面另一边的所有K维数据对应维度上的值。也就是说，我们每选择一个维度进行如上的划分，就会将K维数据空间划分为两个部分，如果我 们继续分别对这两个子K维空间进行如上的划分，又会得到新的子空间，对新的子空间又继续划分，重复以上过程直到每个子空间都不能再划分为止（什么叫不能再划分？）
2. 每次对子空间的划分时，怎样确定在哪个维度上进行划分？最大方差法（max invarince），即每次我们选择维度进行划分时，都选择具有最大方差维度。
3. 在某个维度上进行划分时，怎样确保在这一维度上的划分得到的两个子集合的数量尽量相等，即左子树和右子树中的结点个数尽量相等？在维度i上进行划分时，pivot就选择该维度i上所有数据的*中值*，这样得到的两个子集合数据个数就基本相同了。
4. Kd-Tree的构建算法：
 - 在K维数据集合中选择具有最大方差的维度k，然后在该维度上选择中值m为pivot对该数据集合进行划分，得到两个子集合；同时创建一个树结点node，用于存储；
 - 对两个子集合重复（1）步骤的过程，直至所有子集合都不能再划分为止；如果某个子集合不能再划分时，则将该子集合中的数据保存到叶子结点（leaf node）。

5. 利用Kd-Tree进行最近邻查找的算法：略。
6. 让Kd-tree满足对高维（大于30）数据的索引：best bin first：略。

## SIFT。
### 参考：https://blog.csdn.net/abcjennifer/article/details/7639681/
1. SIFT特征具有尺度不变性和旋转不变性。
2. 构建尺度空间：金字塔。同一层中的每张图片的模糊程度不同，图片大小相同（通过不同sigma值得高斯卷积得到）。上一层的图片是用下一层的图片降采样得到的，长宽分别除以2，再进行高斯卷积得到同一层的其它图片。
3. DOG，同一层的每相邻两张图片作差得到。
4. 进行非极大值抑制。中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点。
5. 利用Hessian矩阵再进行筛选。
6. 由梯度直方图确定特征点的主方向。对特征点邻域的像素点统计梯度方向。
7. 在主方向上选取16x16个像素点，分别计算梯度，将16x16划分成4x4共16个方格，每个方格有4x4个像素，对每个方格统计加权梯度值到8个方向中的一个，所以每个特征点的描述子的维度为4x4x8=128。

## SURF。
### 参考：https://blog.csdn.net/zilanpotou182/article/details/72848311 https://blog.csdn.net/songzitea/article/details/16986423
1. 具有尺度和旋转不变性。具体实现:先使用积分图来计算卷积；然后使用Hessian响应来衡量某点是否为特征点，并创建描述子来描述该特征；以及对整个流程进行一定的优化。
2. 基于Hessian响应的特征检测算子。对于图像中某点X=(x,y)，Hessian矩阵的四项分别代表高斯二阶微分IxxIyyIxy与图像I的对应点进行卷积得到的结果。一个点会有多个σ值下有多个Hessian矩阵。把高斯二阶微分滤波器使用盒滤波器来近似，也就是相当于把高斯滤波器进一步的只分成几块，这样下一步可以使用积分图进行快速计算。这就是说Hessian矩阵的求法，本来是需要与高斯二阶微分进行卷积，现在近似成与盒滤波器进行卷积。
3. 由于我们的特征点需要尺度无关性，所以在进行Hessian矩阵构造前，需要对其进行高斯滤波。生成尺度空间，即图像金字塔。与SIFT不同的是，在进行高斯模糊时，SIFT的高斯模板大小是始终不变的，只是在不同的octave之间改变图片的大小。而在SURF中，图片的大小是一直不变的，不同的octave层得到的待检测图片是改变高斯模糊尺寸大小得到的，当然了，同一个octave中个的图片用到的高斯模板尺度也不同。算法允许尺度空间多层图像同时被处理，不需对图像进行二次抽样，从而提高算法性能。
4. 完成以上的步骤后，就可以求出Hessian矩阵的判别式，也叫Hessian响应。如果某点的Hessian响应在其同层的8邻域点以及上一层和下一层的各9个点，即一共26个三维邻域点内，属于最大或者最小，那么该点就是该区域内的特征点。然后，采用3维线性插值法得到亚像素级的特征点，同时也去掉那些值小于一定阈值的点，增加极值使检测到的特征点数量减少，最终只有几个特征最强点会被检测出来。
5. 确定特征点主方向。在SURF中，不统计其梯度直方图，而是统计特征点领域内的Harr小波特征。统计60度扇形内所有像素点在x(水平)和y(垂直)方向的Haar小波响应总和，并给这些响应值赋高斯权重系数，使得靠近特征点的响应贡献大，而远离特征点的响应贡献小，然后60度范围内的响应相加以形成新的矢量，遍历整个圆形区域，选择最长矢量的方向为该特征点的主方向。
6. 构造SURF特征点描述算子。在SURF中，也是在特征点周围取一个正方形框，该框带方向，方向当然就是第4步检测出来的主方向了。然后把该框分为16个子区域，每个子区域统计25个像素的水平方向和垂直方向的haar小波特征，这里的x(水平)和y(垂直)方向都是相对主方向而言的。该haar小波特征为x(水平)方向值之和，水平方向绝对值之和，垂直方向之和，垂直方向绝对值之和。所以每个子区域都有四个特征维度，总共16个子区域，特征描述子的维度为4x16=64。

## ORB。
### 参考：https://blog.csdn.net/zouzoupaopao229/article/details/526256781
1. ORB算法分为两部分，分别是特征点提取和特征点描述。特征提取是由FAST（Features from  Accelerated Segment Test）算法发展来的，特征点描述是根据BRIEF（Binary Robust IndependentElementary Features）特征描述算法改进的。ORB特征是将FAST特征点的检测方法与BRIEF特征描述子结合起来，并在它们原来的基础上做了改进与优化。
2. ORB算法的特征提取是由FAST算法改进的，这里成为oFAST（FASTKeypoint Orientation）。也就是说，在使用FAST提取出特征点之后，给其定义一个特征点方向，以此来实现特征点的旋转不变形。步骤如下：
 - 粗提取。从图像中选取一点P，以P为圆心画一个半径为3pixel的圆。圆周上如果有连续n个像素点的灰度值比P点的灰度值大或者小，则认为P为特征点。一般n设置为12。为了加快特征点的提取，快速排出非特征点，首先检测1、9、5、13位置上的灰度值，如果P是特征点，那么这四个位置上有3个或3个以上的的像素值都大于或者小于P点的灰度值。如果不满足，则直接排除此点。
 - 机器学习的方法筛选最优特征点。简单来说就是使用ID3算法训练一个决策树，将特征点圆周上的16个像素输入决策树中，以此来筛选出最优的FAST特征点。
 - 非极大值抑制去除局部较密集特征点。为每一个特征点计算出其响应大小。计算方式是特征点P和其周围16个特征点偏差的绝对值和。在比较临近的特征点中，保留响应值较大的特征点，删除其余的特征点。
 - 特征点的尺度不变形。建立金字塔，来实现特征点的多尺度不变性。设置一个比例因子scaleFactor（opencv默认为1.2）和金字塔的层数nlevels（opencv默认为8）。将原图像按比例因子缩小成nlevels幅图像。nlevels幅不同比例的图像提取特征点总和作为这幅图像的oFAST特征点。
 - 特征点的旋转不变性。ORB算法提出使用矩（moment）法来确定FAST特征点的方向。也就是说通过矩来计算特征点以r为半径范围内的质心，特征点坐标到质心形成一个向量作为该特征点的方向。假设角点坐标为原点，则向量的角度即为该特征点的方向。
3. rBRIEF特征描述。rBRIEF特征描述是在BRIEF特征描述的基础上加入旋转因子改进的。
 - BRIEF算法计算出来的是一个二进制串的特征描述符。它是在一个特征点的邻域内，选择n对像素点pi、qi（i=1,2,…,n）。然后比较每个点对的灰度值的大小。如果I(pi)> I(qi)，则生成二进制串中的1，否则为0。所有的点对都进行比较，则生成长度为n的二进制串。一般n取128、256或512，opencv默认为256。
 - 关于在特征点SxS的区域内选取点对的方法，下面的方法效果较好：p和q都符合(0,S平方/25)的高斯分布；
 - 但在旋转大于30°后，BRIEF的匹配率快速降到0左右。
 - 如何改进？旋转不变性改进：steered BRIEF。使用oFast算法计算出的特征点中包括了特征点的方向角度。假设原始的BRIEF算法在特征点SxS（一般S取31）邻域内选取n对点集。经过旋转角度θ旋转，得到新的点对，在新的点集位置上比较点对的大小形成二进制串的描述符。这里需要注意的是，在使用oFast算法是在不同的尺度上提取的特征点。因此，在使用BRIEF特征描述时，要将图像转换到相应的尺度图像上，然后在尺度图像上的特征点处取SxS邻域，然后选择点对并旋转，得到二进制串描述符。
 - 使用steeredBRIEF方法得到的特征描述子具有旋转不变性，但是却在描述符的可区分性上不如原始的BRIEF算法。BRIEF算法计算的描述符的均值在0.5左右，每个描述符的方差较大，可区分性较强。而steeredBRIEF失去了这个特性。（如果二进制串的均值在0.5左右的话，那么这个串有大约相同数目的0和1，那么方差就较大了，这样两个特征点的描述子在相同位置出现相同0或1的概率就较小。）
 - 解决上面这个问题的方法：rBRIEF。ORB论文中使用统计学习的方法来选择点对集合，而不是p和q都符合(0,S平方/25)的高斯分布。首先建立300k个特征点测试集。对于测试集中的每个点，考虑其31x31邻域。在对图像进行高斯平滑之后，使用邻域中的某个点的5x5邻域灰度平均值来代替某个点的值。从上面可知，在31x31的邻域内共有(31-5+1)x(31-5+1)=729个这样的子窗口，那么取点对的方法共有M=265356（728+。。。+1）种，我们就要在这Ｍ种方法中选取256种取法，选择的原则是这256种取法之间的相关性最小。1) 在300k特征点的每个31x31邻域内按M种方法取点对，比较点对大小，形成一个300kxM的二进制矩阵Q。矩阵的每一列代表300k个点按某种取法得到的二进制数。2）对Q矩阵的每一列求取平均值，按照平均值到0.5的距离大小重新对Q矩阵的列向量排序，形成矩阵T。3）将T的第一列向量放到R中。4）取T的下一列向量和R中的所有列向量计算相关性，如果相关系数小于设定的阈值，则将T中的该列向量移至R中。5）按照4）的方式不断进行操作，直到R中的向量数量为256。*通过这种方法就选取了这256种取点对的方法。*接着再用这些取点对的方法取，比较两个点，生成长度为256的二进制串，总共32个字节。
4. ORB算法在具有较稳定的旋转不变性。但是尺度方面效果较差，在增加算法的尺度变换的情况下仍然没有取得较好的结果。和sift相比，ORB使用二进制串作为特征描述，这就造成了高的误匹配率。

## RANSAC。
### 参考：https://blog.csdn.net/luoshixian099/article/details/50217655
1. 寻找一个3x3的变换矩阵，尽量使得满足该变换矩阵的匹配特征点对的数量最多。将第三行第三列的值固定为1，那么该矩阵有8个未知数，一组点对跟该矩阵可以列出两个方程，所以总共需要4组点对才能解出该变换矩阵。
2. 算法步骤如下：
 - 随机从数据集中随机抽出4个样本数据 (此4个样本之间不能共线)，计算出变换矩阵H，记为模型M；
 - 计算数据集中所有数据与模型M的投影误差，若误差小于阈值，加入内点集 I ； 
 - 如果当前内点集 I 元素个数大于最优内点集 I_best , 则更新 I_best = I，同时更新迭代次数k ;
 - 如果迭代次数大于k,则退出 ; 否则迭代次数加1，并重复上述三个步骤；
3. 迭代次数k在不大于最大迭代次数的情况下，是在不断更新而不是固定的；更新方式参考博客。
4. 每一次循环迭代都再次选取4个样本数据，迭代过程中有记录下最优内点集，当超过最大迭代次数时最优内点集就是找到的inlier。

## LSH局部敏感哈希。
### 参考：https://blog.csdn.net/icvpr/article/details/12342159 建议直接看原文算了。。。
1. kd-tree是一个很好算法，但是当数据的维数很高时，kd-tree的性能将变得很差（kd-tree只适用数据在30维以下）。对于高维数据的海量数据近邻查找，局部敏感哈希是一个很好的解决方法。
2. 局部敏感哈希的基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有*很大的概率*任然相邻；而原本不相邻的两个数据，在低维空间中也将有*很大的概率*不相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，避免在高维数据空间中寻找，因为在高维空间中会很耗时。有这样性质的哈希映射称为是局部敏感的。如果我们能够找到这样一些hash functions，使得经过它们的哈希映射变换后，原始空间中相邻的数据落入相同的桶内的话，那么我们在该数据集合中进行近邻查找就变得容易了，我们只需要将查询数据进行哈希映射得到其桶号，然后取出该桶号对应桶内的所有数据，再进行线性匹配即可查找到与查询数据相邻的数据。换句话说，我们通过hash function映射变换操作，将原始数据集合分成了多个子集合，而每个子集合中的数据间是相邻的且该子集合中的元素个数较小，因此将一个在超大集合内查找相邻元素的问题转化为了在一个很小的集合内查找相邻元素的问题，显然计算量下降了很多。
3. 那具有怎样特点的hash functions才能够使得原本相邻的两个数据点经过hash变换后会落入相同的桶内？这些hash function需要满足以下两个条件：1）如果d(x,y) ≤ d1， 则h(x) = h(y)的概率至少为p1；2）如果d(x,y) ≥ d2， 则h(x) = h(y)的概率至多为p2；其中d(x,y)表示x和y之间的距离，d1 < d2， h(x)和h(y)分别表示对x和y进行hash变换。满足以上两个条件的hash functions称为(d1,d2,p1,p2)-sensitive。而通过一个或多个(d1,d2,p1,p2)-sensitive的hash function对原始数据集合进行hashing生成一个或多个hash table的过程称为Locality-sensitive Hashing（LSH）。
4. 使用LSH进行对海量数据建立索引（Hash table）并通过索引来进行近似最近邻查找的过程如下：
 - 离线建立索引。（1）选取满足(d1,d2,p1,p2)-sensitive的LSH hash functions；（2）根据对查找结果的准确率（即相邻的数据被查找到的概率）确定hash table的个数L，每个table内的hash functions的个数K，以及跟LSH hash function自身有关的参数；（3）将所有数据经过LSH hash function哈希到相应的桶内，构成了一个或多个hash table；
 - 在线查找（1）将查询数据经过LSH hash function哈希得到相应的桶号；（2）将桶号中对应的数据取出；（为了保证查找速度，通常只需要取出前2L个数据即可）；（3）计算查询数据与这2L个数据之间的相似度或距离，返回最近邻的数据；
5. LSH为我们提供了一种在海量的高维数据集中查找与查询数据点（query data point）近似最相邻的某个或某些数据点。需要注意的是，LSH并不能保证一定能够查找到与query data point最相邻的数据，而是减少需要匹配的数据点个数的同时保证查找到最近邻的数据点的概率很大。