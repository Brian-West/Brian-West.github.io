### 三种特征提取算法SIFT, SURF, ORB(Fast+Brief)以及RANSAC消除野点影响的OpenCV3实现。
1. 在OpenCV3中，使用SIFT, SURF, ORB三种特征提取算法的第一步都是创建相应的特征提取器，不同的地方在于create()静态方法所在的namespace不同，如下：
```
Ptr<Feature2D> sift_ptr = cv::xfeatures2d::SIFT::create();
Ptr<Feature2D> surf_ptr = cv::xfeatures2d::SURF::create();
Ptr<Feature2D> orb_ptr = cv::ORB::create();
```
这是利用了多态性，SIFT, SURF, ORB都公有继承了Feature2D类。智能指针Ptr，使用Ptr则不用关心delete问题，系统会自动回收不再使用的内存。

2. 接着读取图片，图片内容放在Mat这个数据结构里，img1是待匹配的图片，img2是模板图片。

3. 接下来对每一章图片都调用（sift\_ptr，surf\_ptr，orb\_ptr的）detect方法，检测出关键点，存放在vector<KeyPoint>这个数组里，KeyPoint是对关键点的基础信息的描述，并不是指特征点的描述符descriptor。KeyPoint数据结构如下：
	```
	/** @brief Data structure for salient point detectors.

	The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint
	detectors, such as Harris corner detector, #FAST, %StarDetector, %SURF, %SIFT etc.

	The keypoint is characterized by the 2D position, scale (proportional to the diameter of the
	neighborhood that needs to be taken into account), orientation and some other parameters. The
	keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually
	represented as a feature vector). The keypoints representing the same object in different images
	can then be matched using %KDTree or another method.
	*/
	class CV_EXPORTS_W_SIMPLE KeyPoint
	{
	public:
	    //! the default constructor
	    CV_WRAP KeyPoint();
	    /**
	    @param _pt x & y coordinates of the keypoint
	    @param _size keypoint diameter
	    @param _angle keypoint orientation
	    @param _response keypoint detector response on the keypoint (that is, strength of the keypoint)
	    @param _octave pyramid octave in which the keypoint has been detected
	    @param _class_id object id
	     */
	    KeyPoint(Point2f _pt, float _size, float _angle=-1, float _response=0, int _octave=0, int _class_id=-1);
	    /**
	    @param x x-coordinate of the keypoint
	    @param y y-coordinate of the keypoint
	    @param _size keypoint diameter
	    @param _angle keypoint orientation
	    @param _response keypoint detector response on the keypoint (that is, strength of the keypoint)
	    @param _octave pyramid octave in which the keypoint has been detected
	    @param _class_id object id
	     */
	    CV_WRAP KeyPoint(float x, float y, float _size, float _angle=-1, float _response=0, int _octave=0, int _class_id=-1);

	    size_t hash() const;

	    /**
	    This method converts vector of keypoints to vector of points or the reverse, where each keypoint is
	    assigned the same size and the same orientation.

	    @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB
	    @param points2f Array of (x,y) coordinates of each keypoint
	    @param keypointIndexes Array of indexes of keypoints to be converted to points. (Acts like a mask to
	    convert only specified keypoints)
	    */
	    CV_WRAP static void convert(const std::vector<KeyPoint>& keypoints,
	                                CV_OUT std::vector<Point2f>& points2f,
	                                const std::vector<int>& keypointIndexes=std::vector<int>());
	    /** @overload
	    @param points2f Array of (x,y) coordinates of each keypoint
	    @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB
	    @param size keypoint diameter
	    @param response keypoint detector response on the keypoint (that is, strength of the keypoint)
	    @param octave pyramid octave in which the keypoint has been detected
	    @param class_id object id
	    */
	    CV_WRAP static void convert(const std::vector<Point2f>& points2f,
	                                CV_OUT std::vector<KeyPoint>& keypoints,
	                                float size=1, float response=1, int octave=0, int class_id=-1);

	    /**
	    This method computes overlap for pair of keypoints. Overlap is the ratio between area of keypoint
	    regions' intersection and area of keypoint regions' union (considering keypoint region as circle).
	    If they don't overlap, we get zero. If they coincide at same location with same size, we get 1.
	    @param kp1 First keypoint
	    @param kp2 Second keypoint
	    */
	    CV_WRAP static float overlap(const KeyPoint& kp1, const KeyPoint& kp2);

	    CV_PROP_RW Point2f pt; //!< coordinates of the keypoints
	    CV_PROP_RW float size; //!< diameter of the meaningful keypoint neighborhood
	    CV_PROP_RW float angle; //!< computed orientation of the keypoint (-1 if not applicable);
	                            //!< it's in [0,360) degrees and measured relative to
	                            //!< image coordinate system, ie in clockwise.
	    CV_PROP_RW float response; //!< the response by which the most strong keypoints have been selected. Can be used for the further sorting or 	subsampling
	    CV_PROP_RW int octave; //!< octave (pyramid layer) from which the keypoint has been extracted
	    CV_PROP_RW int class_id; //!< object class (if the keypoints need to be clustered by an object they belong to)
	};
	```
虽然还是提倡看英文原文，但这里我还是稍微翻译一下好了。

4. 调用（sift\_ptr，surf\_ptr，orb\_ptr的）compute方法，计算出每个关键点的描述符descriptor，存放在Mat矩阵里，行数为特征点数量，列数为每一个特征点的特征向量的维度，	SURF中是64个维度，SIFT是128个维度，ORB是32个维度。

5. 之后就可以进行特征点匹配了，我这里只采用了蛮力匹配，即每个特征点都和另一张图片中的所有特征点计算欧式距离，距离最小的即为匹配成功的点。
```
Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce");
```
调用matcher的match方法，将匹配结果存放在vector<DMatch>这个数组里，每一个DMatch表示两个对应特征点的匹配结果，可得到特征点对的欧式距离。DMatch数据结构如下：
	```
	/** @brief Class for matching keypoint descriptors

	query descriptor index, train descriptor index, train image index, and distance between
	descriptors.
	*/
	class CV_EXPORTS_W_SIMPLE DMatch
	{
	public:
	    CV_WRAP DMatch();
	    CV_WRAP DMatch(int _queryIdx, int _trainIdx, float _distance);
	    CV_WRAP DMatch(int _queryIdx, int _trainIdx, int _imgIdx, float _distance);

	    CV_PROP_RW int queryIdx; //!< query descriptor index
	    CV_PROP_RW int trainIdx; //!< train descriptor index
	    CV_PROP_RW int imgIdx;   //!< train image index

	    CV_PROP_RW float distance;

	    // less is better
	    bool operator<(const DMatch &m) const;
	};
	```
虽然还是提倡看英文原文，但这里我还是稍微翻译一下好了。

6. RANSAC。使用RANSAC算法可以区分出inlier和outlier。
```
vector<uchar> sift_inlierMask(sift_matches.size());
Mat sift_H = findHomography(sift_obj, sift_train, cv::RANSAC, 3.0, sift_inlierMask, 100);
```
检测结果放在sift\_inlierMask里，值非0即inlier，值为0即outlier，利用这个信息可以找到`vector<DMatch> sift_matches`中inlier的匹配结果。默认迭代次数为2000，我这里设置为100（最后一个参数）。返回值是一个透视转换矩阵（Perspective Transformation Matrix），将它传进warpPerspective函数中就可以对待匹配图片进行矫正，矫正的依据是模板图片。

7. 完整代码如下。代码中SIFT, SURF, ORB都实现了，用法基本相同。RANSAC部分基本相似，可以只看SIFT之后使用RANSAC就能了解整个过程。

```
#include <iostream>
#include <opencv2/opencv.hpp>
#include <opencv2/xfeatures2d.hpp>

using namespace std;
using namespace cv;

int main() {
	Ptr<Feature2D> sift_ptr = xfeatures2d::SIFT::create();
	Ptr<Feature2D> surf_ptr = xfeatures2d::SURF::create();
	Ptr<Feature2D> orb_ptr = ORB::create();

	//读入图片
	Mat img1 = imread("D:\\Documents\\Visual Studio 2015\\Projects\\SIFT-Test\\2.jpg");
	Mat img2 = imread("D:\\Documents\\Visual Studio 2015\\Projects\\SIFT-Test\\1.jpg");

	//提取特征点，放在KeyPoint的向量容器里
	vector<KeyPoint> sift_key1, sift_key2, surf_key1, surf_key2, orb_key1, orb_key2;
	sift_ptr->detect(img1, sift_key1);
	sift_ptr->detect(img2, sift_key2);

	surf_ptr->detect(img1, surf_key1);
	surf_ptr->detect(img2, surf_key2);
	
	orb_ptr->detect(img1, orb_key1);
	orb_ptr->detect(img2, orb_key2);

	//计算特征点描述符，记录在Mat矩阵里，矩阵大小为N*M，N代表特征点的特征向量的维度，M代表特征点数量。SURF中是64个维度
	Mat sift_desc1, sift_desc2, surf_desc1, surf_desc2, orb_desc1, orb_desc2;
	sift_ptr->compute(img1, sift_key1, sift_desc1);
	sift_ptr->compute(img2, sift_key2, sift_desc2);

	surf_ptr->compute(img1, surf_key1, surf_desc1);
	surf_ptr->compute(img2, surf_key2, surf_desc2);

	orb_ptr->compute(img1, orb_key1, orb_desc1);
	orb_ptr->compute(img2, orb_key2, orb_desc2);

	//匹配图片
	Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce");
	vector<DMatch> sift_matches, surf_matches, orb_matches;//存放匹配结果，一个DMatch表示两个对应特征点的匹配结果，可得到特征点的欧式距离
	matcher->match(sift_desc1, sift_desc2, sift_matches);

	matcher->match(surf_desc1, surf_desc2, surf_matches);

	matcher->match(orb_desc1, orb_desc2, orb_matches);

	//绘制匹配出的特征点，“画在”Mat里
	Mat sift_show;
	drawMatches(img1, sift_key1, img2, sift_key2, sift_matches, sift_show);
	imshow("SIFT匹配结果展示", sift_show);
	imwrite("SIFT蛮力匹配.jpg", sift_show);
	//进行RANSAC野点(Outlier)消除
	vector<Point2f> sift_obj;
	vector<Point2f> sift_train;
	for (int i = 0;i < sift_matches.size();i++) {
		sift_obj.push_back(sift_key1[sift_matches[i].queryIdx].pt);
		sift_train.push_back(sift_key2[sift_matches[i].trainIdx].pt);
	}
	vector<uchar> sift_inlierMask(sift_matches.size());
	Mat sift_H = findHomography(sift_obj, sift_train, cv::RANSAC, 3.0, sift_inlierMask, 100);
	vector<DMatch> sift_inliers;
	for (int i = 0;i < sift_inlierMask.size();i++) {
		if (sift_inlierMask[i]) {
			sift_inliers.push_back(sift_matches[i]);
		}
	}
	sift_matches.swap(sift_inliers);

	Mat sift_show2;
	drawMatches(img1, sift_key1, img2, sift_key2, sift_matches, sift_show2);
	imshow("SIFT+RANSAC之后的匹配结果展示", sift_show2);
	imwrite("SIFT蛮力匹配+RANSAC.jpg", sift_show2);

	//将待匹配的图片（第一张）按照模板图片（第二张）的角度进行纠正（透视变换）
	Size size = img2.size();
	Mat sift_descImg = Mat::zeros(size, CV_8UC3);
	warpPerspective(img1, sift_descImg, sift_H, size);
	imshow("SIFT+RANSAC加转移矩阵矫正后（透视变换）的图片", sift_descImg);
	imwrite("SIFT蛮力匹配+RANSAC+透视变换.jpg", sift_descImg);

	//
	Mat surf_show;
	drawMatches(img1, surf_key1, img2, surf_key2, surf_matches, surf_show);
	imshow("SURF匹配结果展示", surf_show);
	imwrite("SURF蛮力匹配.jpg", surf_show);
	//进行RANSAC野点(Outlier)消除
	vector<Point2f> surf_obj;
	vector<Point2f> surf_train;
	for (int i = 0;i < surf_matches.size();i++) {
		surf_obj.push_back(surf_key1[surf_matches[i].queryIdx].pt);
		surf_train.push_back(surf_key2[surf_matches[i].trainIdx].pt);
	}
	vector<uchar> surf_inlierMask(surf_matches.size());
	Mat surf_H = findHomography(surf_obj, surf_train, cv::RANSAC, 3.0, surf_inlierMask, 100);
	vector<DMatch> surf_inliers;
	for (int i = 0;i < surf_inlierMask.size();i++) {
		if (surf_inlierMask[i]) {
			surf_inliers.push_back(surf_matches[i]);
		}
	}
	surf_matches.swap(surf_inliers);

	Mat surf_show2;
	drawMatches(img1, surf_key1, img2, surf_key2, surf_matches, surf_show2);
	imshow("SURF+RANSAC之后的匹配结果展示", surf_show2);
	imwrite("SURF蛮力匹配+RANSAC.jpg", surf_show2);

	//将待匹配的图片（第一张）按照模板图片（第二张）的角度进行纠正（透视变换）
	Mat surf_descImg = Mat::zeros(size, CV_8UC3);
	warpPerspective(img1, surf_descImg, surf_H, size);
	imshow("SURF+RANSAC加转移矩阵矫正后（透视变换）的图片", surf_descImg);
	imwrite("SURF蛮力匹配+RANSAC+透视变换.jpg", surf_descImg);

	//
	Mat orb_show;
	drawMatches(img1, orb_key1, img2, orb_key2, orb_matches, orb_show);
	imshow("ORB匹配结果展示", orb_show);
	imwrite("ORB蛮力匹配.jpg", orb_show);
	//进行RANSAC野点(Outlier)消除
	vector<Point2f> orb_obj;
	vector<Point2f> orb_train;
	for (int i = 0;i < orb_matches.size();i++) {
		orb_obj.push_back(orb_key1[orb_matches[i].queryIdx].pt);
		orb_train.push_back(orb_key2[orb_matches[i].trainIdx].pt);
	}
	vector<uchar> orb_inlierMask(orb_matches.size());
	Mat orb_H = findHomography(orb_obj, orb_train, cv::RANSAC, 3.0, orb_inlierMask, 100);
	vector<DMatch> orb_inliers;
	for (int i = 0;i < orb_inlierMask.size();i++) {
		if (orb_inlierMask[i]) {
			orb_inliers.push_back(orb_matches[i]);
		}
	}
	orb_matches.swap(orb_inliers);

	Mat orb_show2;
	drawMatches(img1, orb_key1, img2, orb_key2, orb_matches, orb_show2);
	imshow("ORB+RANSAC之后的匹配结果展示", orb_show2);
	imwrite("ORB蛮力匹配+RANSAC.jpg", orb_show2);

	//将待匹配的图片（第一张）按照模板图片（第二张）的角度进行纠正（透视变换）
	Mat orb_descImg = Mat::zeros(size, CV_8UC3);
	warpPerspective(img1, orb_descImg, orb_H, size);
	imshow("ORB+RANSAC加转移矩阵矫正后（透视变换）的图片", orb_descImg);
	imwrite("ORB蛮力匹配+RANSAC+透视变换.jpg", orb_show2);

	//等待任意按键按下即销毁窗口
	waitKey(0);
}
```
